{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import re\n",
    "import ray\n",
    "import codecs\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from random import random, shuffle, randint\n",
    "\n",
    "DELIMS = {\n",
    "    'section': '~',\n",
    "    'category': '`',\n",
    "    'keywords': '^',\n",
    "    'title': '@',\n",
    "    'body': '}'\n",
    "}\n",
    "\n",
    "PRONOUN_LIST = ['I', 'Me', 'We', 'You', 'He', 'She',\n",
    "                'It', 'Him', 'Her', 'Them', 'They']\n",
    "\n",
    "PRONOUNS = set(PRONOUN_LIST + [x.lower() for x in PRONOUN_LIST])\n",
    "\n",
    "\n",
    "def encode_keywords(csv_path, model='en_core_web_sm',\n",
    "                    category_field=None,\n",
    "                    keywords_field=None,\n",
    "                    title_field=None,\n",
    "                    body_field=None,\n",
    "                    keyword_gen='title',\n",
    "                    keyword_sep=',',\n",
    "                    dropout=0.5,\n",
    "                    repeat=3,\n",
    "                    max_keywords=3,\n",
    "                    keyword_length_max=20,\n",
    "                    out_path='csv_encoded.txt',\n",
    "                    start_token=\"<|startoftext|>\",\n",
    "                    end_token=\"<|endoftext|>\"):\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    with open(csv_path, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        try:\n",
    "            row = None\n",
    "            for row in reader:\n",
    "                if not reader:\n",
    "                    break\n",
    "                data_list.append(row)\n",
    "        except Exception as e:\n",
    "            print(row)\n",
    "\n",
    "    shuffle(data_list)\n",
    "\n",
    "    # https://stackoverflow.com/a/434328\n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "    num_threads = multiprocessing.cpu_count() * 2  # colocate 2 processes per thread\n",
    "    print(\"Starting up {} Workers\".format(num_threads))\n",
    "    encoders = [Encoder.remote(model, category_field,\n",
    "                               keywords_field,\n",
    "                               title_field,\n",
    "                               body_field,\n",
    "                               keyword_gen,\n",
    "                               keyword_sep,\n",
    "                               repeat,\n",
    "                               max_keywords,\n",
    "                               keyword_length_max,\n",
    "                               start_token,\n",
    "                               end_token,\n",
    "                               DELIMS,\n",
    "                               PRONOUNS) for _ in range(num_threads)]\n",
    "\n",
    "    with open(out_path, 'w', encoding='utf-8', errors='ignore') as w:\n",
    "        pbar = tqdm(total=len(data_list), smoothing=0)\n",
    "        for chunk in chunker(data_list, num_threads):\n",
    "            results = ray.get([c.generate_encoded_text.remote(row)\n",
    "                               for c, row in list(zip(encoders, chunk))])\n",
    "\n",
    "            # unnest and randomize results\n",
    "            results = list(chain.from_iterable(results))\n",
    "            shuffle(results)\n",
    "            \n",
    "            for result in results:\n",
    "                w.write(result)\n",
    "\n",
    "            pbar.update(num_threads)\n",
    "        pbar.close()\n",
    "\n",
    "\n",
    "@ray.remote(num_cpus=0.5)\n",
    "class Encoder(object):\n",
    "    def __init__(self, model, category_field,\n",
    "                 keywords_field,\n",
    "                 title_field,\n",
    "                 body_field,\n",
    "                 keyword_gen,\n",
    "                 keyword_sep,\n",
    "                 repeat,\n",
    "                 max_keywords,\n",
    "                 keyword_length_max,\n",
    "                 start_token,\n",
    "                 end_token,\n",
    "                 DELIMS,\n",
    "                 PRONOUNS):\n",
    "        self.nlp = spacy.load(model)\n",
    "        self.pattern = re.compile('\\W+')\n",
    "\n",
    "        self.category_field = category_field\n",
    "        self.keywords_field = keywords_field\n",
    "        self.title_field = title_field\n",
    "        self.body_field = body_field\n",
    "        self.keyword_gen = keyword_gen\n",
    "        self.keyword_sep = keyword_sep\n",
    "        self.repeat = repeat\n",
    "        self.max_keywords = max_keywords\n",
    "        self.keyword_length_max = keyword_length_max\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.DELIMS = DELIMS\n",
    "        self.PRONOUNS = PRONOUNS\n",
    "\n",
    "    def build_section(self, section, text):\n",
    "        if text is None:\n",
    "            return ''\n",
    "        return self.DELIMS['section'] + self.DELIMS[section] + text\n",
    "\n",
    "    def generate_encoded_text(self, row):\n",
    "\n",
    "        nlp = self.nlp\n",
    "        pattern = self.pattern\n",
    "\n",
    "        # category should be normalized to account for user input\n",
    "        category = re.sub(\n",
    "            pattern, '-', row[self.category_field].lower().strip()) if self.category_field is not None else None\n",
    "\n",
    "        title = row[self.title_field] if self.title_field is not None else None\n",
    "        body = row[self.body_field] if self.body_field is not None else None\n",
    "\n",
    "        if self.keywords_field is None:\n",
    "            # Generate the keywords using spacy\n",
    "            # replace smart quotes first for better tokenization\n",
    "            text = re.sub(u'[\\u2018\\u2019]', \"'\",\n",
    "                          (re.sub(u'[\\u201c\\u201d]', '\"', row[self.keyword_gen])))\n",
    "            doc = nlp(text)\n",
    "            keywords_pos = [chunk.text if chunk.pos_ == 'NOUN'\n",
    "                            else chunk.lemma_ if chunk.pos_ in ['VERB', 'ADJ', 'ADV']\n",
    "                            else 'I'\n",
    "                            for chunk in doc\n",
    "                            if not chunk.is_stop\n",
    "                            ]\n",
    "            keywords_ents = [re.sub(' ', '-', chunk.text)\n",
    "                             for chunk in doc.ents]\n",
    "            keywords_compounds = [re.sub(' ', '-', chunk.text)\n",
    "                                  for chunk in doc.noun_chunks\n",
    "                                  if len(chunk.text) < self.keyword_length_max]\n",
    "\n",
    "            keywords = list(set(keywords_pos +\n",
    "                                keywords_ents +\n",
    "                                keywords_compounds) - self.PRONOUNS)  # dedupe\n",
    "            keywords = [x.lower() for x in keywords]\n",
    "        else:\n",
    "            keywords = [keyword.strip()\n",
    "                        for keyword in row[self.keywords_field].split(self.keyword_sep)]\n",
    "            keywords = list(set(keywords))\n",
    "\n",
    "        encoded_texts = []\n",
    "        for _ in range(self.repeat):\n",
    "            new_keywords = keywords\n",
    "            shuffle(new_keywords)\n",
    "            new_keywords = \" \".join(\n",
    "                new_keywords[:randint(0, self.max_keywords)])\n",
    "\n",
    "            encoded_texts.append(self.start_token +\n",
    "                                 self.build_section('category', category) +\n",
    "                                 self.build_section('keywords', new_keywords) +\n",
    "                                 self.build_section('title', title) +\n",
    "                                 self.build_section('body', body) +\n",
    "                                 self.end_token + \"\\n\")\n",
    "        return encoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-24 21:00:34,839\tWARNING worker.py:673 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-12-24 21:00:34,843\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2019-12-24 21:00:34,846\tINFO resource_spec.py:216 -- Starting Ray with 43.07 GiB memory available for workers and up to 0.09 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.31.24.225',\n",
       " 'redis_address': '172.31.24.225:11318',\n",
       " 'object_store_address': '/tmp/ray/session_2019-12-24_21-00-34_842513_21821/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-12-24_21-00-34_842513_21821/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2019-12-24_21-00-34_842513_21821'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "n_categories = 7\n",
    "\n",
    "ray.init(object_store_memory=100 * 1000000,\n",
    "         redis_max_memory=100 * 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1539 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up 8 Workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1544it [00:20, 74.28it/s]                          \n"
     ]
    }
   ],
   "source": [
    "encode_keywords(csv_path='data/love_by_category.csv',\n",
    "                category_field='category',\n",
    "                body_field='body',\n",
    "                keyword_gen='body',\n",
    "                out_path='data/csv_encoded.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
